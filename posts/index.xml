<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Posts on Well Organized Bits</title><link>https://mbezjak.github.io/posts/</link><description>Recent content in Posts on Well Organized Bits</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 10 Nov 2022 14:09:01 +0100</lastBuildDate><atom:link href="https://mbezjak.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Routing Slf4j Events To Mulog</title><link>https://mbezjak.github.io/posts/routing-slf4j-events-to-mulog/</link><pubDate>Thu, 10 Nov 2022 14:09:01 +0100</pubDate><guid>https://mbezjak.github.io/posts/routing-slf4j-events-to-mulog/</guid><description>mulog plays to Clojure&amp;rsquo;s strengths such as declaring, using and manipulating data vs. composing and parsing strings. That&amp;rsquo;s why I like using it as a logging library. However, backend services might use other Java libraries that only use what&amp;rsquo;s available in the JVM ecosystem. Best case scenario is that backend ends up using at least two logging libraries: mulog and slf4j. Each one has to be configured to work properly. Even if they are, some questions still remain:</description></item><item><title>Avoiding Type Problems When Logging to ElasticSearch</title><link>https://mbezjak.github.io/posts/avoiding-type-problems-when-logging-to-es/</link><pubDate>Wed, 09 Nov 2022 13:38:22 +0100</pubDate><guid>https://mbezjak.github.io/posts/avoiding-type-problems-when-logging-to-es/</guid><description>If you&amp;rsquo;re sending your log events to ElasticSearch, you might notice that ElasticSearch sometimes fails to index some events due to type problems. A single ES index cannot contain two documents having the same key but different value types. For example, {&amp;quot;user&amp;quot;: 1} (integer) and {&amp;quot;user&amp;quot;: &amp;quot;jane@example.com&amp;quot;} (string) cannot be written to the same ES index.
mulog is an excellent logging library for Clojure. It can also send your events to ElasticSearch.</description></item><item><title>Mulog Publisher for Nicer Local Development</title><link>https://mbezjak.github.io/posts/mulog-local-development-publisher/</link><pubDate>Wed, 09 Nov 2022 12:22:17 +0100</pubDate><guid>https://mbezjak.github.io/posts/mulog-local-development-publisher/</guid><description>mulog has a couple of console publishers ready for use:
Simple console publisher: output in EDN format Advanced console publisher: output in JSON format Both can be used as publishers for local development (either running locally or executing tests. The problem is that their output is not something pleasant to look at. Even if the output is pretty printed. Here is a contrived example, with just 5 events.
{:mulog/event-name :company.system.slf4j-init/slf4j, :mulog/timestamp 1668079374515, :mulog/trace-id #mulog/flake &amp;#34;4mNpLawLr6vG7UiZo44Uxi4U-MgZ1ugS&amp;#34;, :mulog/namespace &amp;#34;company.</description></item><item><title>Make Mulog Play Nice With Kaocha</title><link>https://mbezjak.github.io/posts/make-mulog-play-nice-with-kaocha/</link><pubDate>Fri, 04 Nov 2022 17:16:10 +0100</pubDate><guid>https://mbezjak.github.io/posts/make-mulog-play-nice-with-kaocha/</guid><description>mulog is an excellent logging library for Clojure. It uses agents to process events asynchronously. This makes sense for deployed services.
But what about executing (integration, functional, etc.) tests? Some tests runners (e.g. Kaocha) try to capture the test output while the test is executing and provide that output in case the test failed. Do those two libraries play well together? Not really. The output captured cannot be relied upon:</description></item><item><title>Start the System Before Executing Integration Tests in Kaocha</title><link>https://mbezjak.github.io/posts/kaocha-start-system-hook/</link><pubDate>Fri, 04 Nov 2022 15:59:25 +0100</pubDate><guid>https://mbezjak.github.io/posts/kaocha-start-system-hook/</guid><description>Kaocha is a very good tests runner for Clojure. The documentation is really good.
What I wanted to do recently is to start (and stop) the system before (and after) running the integration tests. How to do that with hooks? If the project has unit tests separated from integration tests then this solution should work.
tests.edn
#kaocha/v1 {:tests [{:id :unit :test-paths [&amp;#34;test/unit&amp;#34;]} {:id :integration :test-paths [&amp;#34;test/integration&amp;#34;]}] :plugins [:hooks] :kaocha.hooks/wrap-run [kaocha-hooks/wrap-tests]} test/kaocha_hooks.</description></item></channel></rss>